<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Neural Network | D</title><meta name="author" content="Lavoisier"><meta name="copyright" content="Lavoisier"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Neural Network 神经网络简介什么是神经网络神经网络是一种模仿人脑神经系统工作原理的计算模型，由大量的相互连接的节点（神经元）组成。这些节点通过调整连接强度（权重）来学习并执行各种复杂的任务，如图像识别、语音处理、自然语言处理等。神经网络的结构包括输入层、隐藏层和输出层，通过反向传播算法不断优化权重，最终得到一个能够很好完成特定任务的模型。 神经网络的发展历史神经网络的概念最早可以追溯">
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta property="og:type" content="article">
<meta property="og:title" content="Neural Network">
<meta property="og:url" content="http://lavoisier7.github.io/2024/11/22/Neural_Network/index.html">
<meta property="og:site_name" content="D">
<meta property="og:description" content="Neural Network 神经网络简介什么是神经网络神经网络是一种模仿人脑神经系统工作原理的计算模型，由大量的相互连接的节点（神经元）组成。这些节点通过调整连接强度（权重）来学习并执行各种复杂的任务，如图像识别、语音处理、自然语言处理等。神经网络的结构包括输入层、隐藏层和输出层，通过反向传播算法不断优化权重，最终得到一个能够很好完成特定任务的模型。 神经网络的发展历史神经网络的概念最早可以追溯">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://lavoisier7.github.io/img/Yoimiya.jpg">
<meta property="article:published_time" content="2024-11-22T08:41:27.000Z">
<meta property="article:modified_time" content="2025-05-09T09:05:18.457Z">
<meta property="article:author" content="Lavoisier">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="NN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lavoisier7.github.io/img/Yoimiya.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Neural Network",
  "url": "http://lavoisier7.github.io/2024/11/22/Neural_Network/",
  "image": "http://lavoisier7.github.io/img/Yoimiya.jpg",
  "datePublished": "2024-11-22T08:41:27.000Z",
  "dateModified": "2025-05-09T09:05:18.457Z",
  "author": [
    {
      "@type": "Person",
      "name": "Lavoisier",
      "url": "http://lavoisier7.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/Yoimiya.jpg"><link rel="canonical" href="http://lavoisier7.github.io/2024/11/22/Neural_Network/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-QFGRX15K8W"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'G-QFGRX15K8W')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'G-QFGRX15K8W', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":120,"languages":{"author":"作者: Lavoisier","link":"链接: ","source":"来源: D","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Neural Network',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/Yoimiya.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/bow.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/Yoimiya.jpg" alt="Logo"><span class="site-name">D</span></a><a class="nav-page-title" href="/"><span class="site-name">Neural Network</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Neural Network</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-22T08:41:27.000Z" title="发表于 2024-11-22 16:41:27">2024-11-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-09T09:05:18.457Z" title="更新于 2025-05-09 17:05:18">2025-05-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Science/">Computer Science</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/11/22/Neural_Network/#post-comment"><span class="waline-comment-count" data-path="/2024/11/22/Neural_Network/"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>Neural Network</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="神经网络简介"><a href="#神经网络简介" class="headerlink" title="神经网络简介"></a>神经网络简介</h2><h3 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h3><p>神经网络是一种模仿人脑神经系统工作原理的计算模型，由大量的相互连接的节点（神经元）组成。这些节点通过调整连接强度（权重）来学习并执行各种复杂的任务，如图像识别、语音处理、自然语言处理等。神经网络的结构包括输入层、隐藏层和输出层，通过反向传播算法不断优化权重，最终得到一个能够很好完成特定任务的模型。</p>
<h3 id="神经网络的发展历史"><a href="#神经网络的发展历史" class="headerlink" title="神经网络的发展历史"></a>神经网络的发展历史</h3><p>神经网络的概念最早可以追溯到20世纪40年代，当时麦卡洛克和皮茨提出了最早的神经网络模型——感知机。此后，神经网络经历了几个重要的发展阶段：</p>
<ul>
<li>1980年代：反向传播算法的提出，使得多层神经网络的训练成为可能。</li>
<li>1990年代：支持向量机（SVM）的兴起，一度对神经网络构成挑战。</li>
<li>2000年代：GPU的出现，大幅提高了神经网络的训练速度，深度学习开始流行。</li>
<li>近年来：随着计算能力的不断增强和大规模数据的积累，神经网络模型不断发展，广泛应用于各个领域。</li>
</ul>
<h3 id="神经网络的组成部分"><a href="#神经网络的组成部分" class="headerlink" title="神经网络的组成部分"></a>神经网络的组成部分</h3><p>神经网络主要由以下几个部分组成：</p>
<ul>
<li>输入层：接收输入数据。</li>
<li>隐藏层：进行特征提取和模式识别。</li>
<li>输出层：产生输出结果。</li>
<li>连接权重：连接神经元之间的权重，通过训练不断调整。</li>
<li>激活函数：决定神经元的输出，增加模型的非线性。</li>
<li>损失函数：定义预测值与真实值之间的差距。</li>
<li>优化算法：不断优化网络参数，如梯度下降算法。</li>
</ul>
<h3 id="神经网络与LLM的关系"><a href="#神经网络与LLM的关系" class="headerlink" title="神经网络与LLM的关系"></a>神经网络与LLM的关系</h3><p>LLM的兴起进一步推动了神经网络技术在自然语言处理方面的应用。LLM本质上也是基于神经网络的模型，利用海量文本数据进行预训练，学习到了强大的自然语言理解和生成能力。常用的神经网络架构如Transformer，以及新兴架构如Megatron等，都被用于构建LLM。这些模型在各种下游任务上表现优异，推动了神经网络在实际应用中的广泛应用。</p>
<h2 id="NN分类及应用"><a href="#NN分类及应用" class="headerlink" title="NN分类及应用"></a>NN分类及应用</h2><p>神经网络有多种不同的分类方式，主要包括：</p>
<p><strong>按结构分</strong>：</p>
<ul>
<li>前馈神经网络：最简单的神经网络结构，信息从输入层向输出层单向传播。</li>
<li>循环神经网络（recurrent neural network, RNN）：能够处理序列数据，捕捉数据中的时序信息。</li>
<li>卷积神经网络（convolutional neural network, CNN）：特别适用于处理图像数据，通过卷积层和池化层提取图像特征。</li>
<li>递归神经网络：处理树形或图形结构的数据，如自然语言处理中的句法分析。</li>
</ul>
<p><strong>按学习方式分</strong>：</p>
<ul>
<li>监督学习神经网络：需要标记数据进行训练，如图像分类、语音识别等。</li>
<li>无监督学习神经网络：不需要标记数据，如聚类、降维等。</li>
<li>强化学习神经网络：通过与环境交互来学习最佳策略，如机器人控制、游戏AI等。</li>
</ul>
<p><strong>按应用领域分</strong>：</p>
<ul>
<li>图像识别&#x2F;计算机视觉：如人脸识别、自动驾驶等。</li>
<li>自然语言处理：如机器翻译、情感分析等。</li>
<li>语音识别：如语音助手、语音搜索等。</li>
<li>推荐系统：如电商推荐、视频推荐等。</li>
<li>金融&#x2F;股票预测：如市场趋势分析、风险管理等。</li>
<li>医疗诊断：如疾病预测、影像识别等。</li>
<li>机器人控制：如运动规划、自主导航等。</li>
</ul>
<h3 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h3><p><strong>一、定义</strong></p>
<p>前馈神经网络（Feed-Forward Neural Network，简称FNN）是一种基本且广泛应用的人工神经网络结构。它是最简单的一种神经网络，其各神经元分层排列，每个神经元只与前一层的神经元相连，接收前一层的输出，并输出给下一层，各层间没有反馈。</p>
<p><strong>二、结构</strong></p>
<p>前馈神经网络主要由输入层、隐藏层和输出层组成：</p>
<ol>
<li><strong>输入层</strong>：从外部接收输入数据，并将其传输给网络的第一层。</li>
<li><strong>隐藏层</strong>：对输入信号进行处理和特征提取，可以有一层或多层。每个隐层由多个神经元（或节点）组成，并且与前一层和后一层的神经元全连接。</li>
<li><strong>输出层</strong>：产生最终的输出结果。输出层的神经元个数取决于问题的类型，例如，对于二分类问题，输出层通常只有一个神经元；对于多分类问题，输出层的神经元个数等于类别的数量。</li>
</ol>
<p><strong>三、工作原理</strong></p>
<ol>
<li><strong>输入数据</strong>：首先进入输入层。</li>
<li><strong>权重和偏置</strong>：输入数据通过权重和偏置传递到隐藏层。权重控制信号在神经元之间的传递强度，偏置用于调整输入信号的加权总和。</li>
<li><strong>隐藏层处理</strong>：隐藏层中的节点对输入进行加权求和，并通过激活函数进行非线性转换。</li>
<li><strong>输出层输出</strong>：输出层接收到经过隐藏层处理的信号，并产生最终的输出。</li>
</ol>
<p><strong>四、激活函数</strong></p>
<p>激活函数在人工神经网络中起着至关重要的作用，它们负责将神经元的输入映射到输出端，并引入非线性因素，使得神经网络能够学习和模拟复杂的函数。常见的激活函数包括：</p>
<ol>
<li><strong>Sigmoid函数</strong>：输出范围在（0,1）之间，适合用于二元分类问题。它连续且可微，但在函数的两端导数接近于0，可能导致梯度消失问题。</li>
<li><strong>Tanh函数</strong>：输出范围在（-1,1）之间，具有零中心化特性，可以加速神经网络的训练过程。与Sigmoid函数一样，Tanh函数也是连续且可微的，但在深层网络中也可能遇到梯度消失的问题。</li>
<li><strong>ReLU函数</strong>：对于所有正输入值，ReLU函数的输出等于输入值本身，这保持了其线性特性，避免了Sigmoid和Tanh函数在输入值较大或较小时的饱和问题。ReLU函数计算效率高，但在输入为负数时，输出恒为0，可能导致“死神经元”问题。</li>
<li><strong>Leaky ReLU和PReLU</strong>：作为ReLU的变体，它们在输入为负数时引入非零斜率，以缓解“神经元死亡”问题。</li>
</ol>
<p><strong>五、训练过程</strong></p>
<p>前馈神经网络的训练过程通常使用反向传播算法，该算法基于梯度下降法，通过计算损失函数对网络中的参数的导数来更新参数，以使网络的预测结果与实际结果更加接近。反向传播分为两个步骤：</p>
<ol>
<li><strong>前向传播</strong>：输入数据通过网络的每一层进行计算，得到预测结果。</li>
<li><strong>反向传播</strong>：通过比较预测结果与实际结果之间的差异，计算损失函数。然后，从输出层开始，使用链式法则计算损失函数对每个层中的权重和偏置的导数，并使用梯度下降法来更新每个参数，减小损失函数的值。</li>
</ol>
<p><strong>六、应用场景</strong></p>
<p>前馈神经网络适用于许多任务，如分类、回归和模式识别等。它还可以与其他模型结合使用，如CNN+Transformer，以进一步提高模型的性能。</p>
<p>综上所述，前馈神经网络是一种基本且强大的人工神经网络结构，通过多层连接的神经元处理输入数据并生成输出。它在许多领域都有广泛的应用，并随着技术的不断发展而不断完善。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>RNN是一种专门用于处理序列数据的神经网络结构，它能够在序列的演进方向上进行递归，并通过所有节点（循环单元）的链式连接来捕捉序列中的时序信息和语义信息。RNN的提出基于记忆模型的想法，期望网络能够记住前面出现的特征，并依据这些特征推断后续的结果。</p>
<p>RNN的核心在于其循环结构，这一结构允许信息在不同时间步之间传递。在每个时间步，RNN接收当前的输入数据（如一个词的嵌入表示）和前一个时间步的隐藏状态，然后生成一个新的隐藏状态。这个新的隐藏状态不仅包含了当前时间步的信息，还融合了之前所有时间步的信息，因此RNN能够捕捉到序列数据中的上下文信息。</p>
<p>RNN的基本结构包括输入层、隐藏层和输出层。其中，隐藏层的状态会随时间步更新，并作为下一时间步的输入之一。这种循环连接使得RNN具有记忆能力，能够捕捉序列中的长距离依赖关系。</p>
<p>RNN的隐藏状态更新公式为：</p>
<p>$$<br>h_t &#x3D; \text{tanh}(W_{xh}x_t + W_{hh}h_{t-1} + b_h)<br>$$</p>
<p>其中，$x_t$是当前时间步的输入，$h_t$是当前时间步的隐藏状态，$h_{t-1}$是前一时间步的隐藏状态，$W_{xh}$和$W_{hh}$是权重矩阵，$b_h$是偏置项，$\text{tanh}$是激活函数。</p>
<p>RNN的权重参数是共享的，即每个时间步都使用相同的权重矩阵。这种权重共享机制可以确保不同时间步的数据都使用相同的模型进行处理，从而保持模型的参数数量相对较小。同时，权重共享也使得RNN能够处理任意长度的序列数据，因为无论序列长度如何变化，模型的结构和参数都保持不变。</p>
<p>RNN的前向传播过程是按照时间步的顺序逐步进行的。在每个时间步，根据当前的输入和前一时间步的隐藏状态计算出新的隐藏状态和输出。</p>
<p>具体过程如下：</p>
<p>输入层接收数据：在每个时间步，RNN的输入层接收当前的输入数据。这个数据可以是序列中的一个元素，如文本中的一个词或时间序列中的一个数据点。<br>隐藏层计算隐藏状态：隐藏层接收当前的输入数据和前一个时间步的隐藏状态，并通过权重矩阵和激活函数计算出新的隐藏状态。这个新的隐藏状态不仅包含了当前时间步的信息，还融合了之前所有时间步的信息。<br>输出层生成输出：输出层根据当前的隐藏状态和权重矩阵计算出输出。这个输出可以是序列中的下一个元素、分类结果或其他任务相关的输出。<br>而反向传播过程则是为了更新模型的权重参数，以最小化损失函数。在反向传播过程中，需要计算损失函数关于每个时间步隐藏状态和权重的梯度，并使用这些梯度来更新权重参数。</p>
<h4 id="一、按输入输出结构分类"><a href="#一、按输入输出结构分类" class="headerlink" title="一、按输入输出结构分类"></a>一、按输入输出结构分类</h4><h5 id="1-N-vs-N-RNN"><a href="#1-N-vs-N-RNN" class="headerlink" title="1. N vs N - RNN"></a>1. N vs N - RNN</h5><ul>
<li><strong>结构特点</strong>：输入和输出序列是等长的。每个时间步的输入都对应一个输出。</li>
<li><strong>工作原理</strong>：网络在每个时间步都接收输入并产生输出，同时隐藏状态在时间步之间传递。</li>
<li><strong>应用场景</strong>：由于输入输出等长，适用于生成等长度的合辙诗句、文本对文本的对齐等任务。</li>
</ul>
<h5 id="2-N-vs-1-RNN"><a href="#2-N-vs-1-RNN" class="headerlink" title="2. N vs 1 - RNN"></a>2. N vs 1 - RNN</h5><ul>
<li><strong>结构特点</strong>：输入是一个序列，输出是一个单独的值或向量。</li>
<li><strong>工作原理</strong>：网络接收序列输入，并在序列处理完毕后输出一个总结性的结果。</li>
<li><strong>应用场景</strong>：情感分析（整个句子的情感倾向）、序列分类（如文本分类、语音识别中的词识别）等。</li>
</ul>
<h5 id="3-1-vs-N-RNN"><a href="#3-1-vs-N-RNN" class="headerlink" title="3. 1 vs N - RNN"></a>3. 1 vs N - RNN</h5><ul>
<li><strong>结构特点</strong>：输入不是序列，而是一个单独的值或向量，输出是一个序列。</li>
<li><strong>工作原理</strong>：网络根据单个输入生成一个序列输出。</li>
<li><strong>应用场景</strong>：图像描述生成（根据图像生成描述性文本）、音乐生成（根据一个主题或风格生成音乐序列）等。</li>
</ul>
<h5 id="4-N-vs-M-RNN（seq2seq）"><a href="#4-N-vs-M-RNN（seq2seq）" class="headerlink" title="4. N vs M - RNN（seq2seq）"></a>4. N vs M - RNN（seq2seq）</h5><ul>
<li><strong>结构特点</strong>：输入和输出序列的长度可以不同。通常包括编码器和解码器两部分。</li>
<li><strong>工作原理</strong>：编码器将输入序列编码为一个固定长度的向量（也称为上下文向量或隐藏状态），解码器则根据这个向量生成输出序列。</li>
<li><strong>应用场景</strong>：机器翻译（将一种语言的句子翻译成另一种语言）、文本摘要（将长文本压缩为短摘要）、问答系统（根据问题生成答案）等。</li>
</ul>
<h4 id="二、按内部构造分类"><a href="#二、按内部构造分类" class="headerlink" title="二、按内部构造分类"></a>二、按内部构造分类</h4><h5 id="1-传统RNN"><a href="#1-传统RNN" class="headerlink" title="1. 传统RNN"></a>1. 传统RNN</h5><ul>
<li><strong>结构特点</strong>：隐藏层的状态是循环的，能够保存和传递之前时间步的信息。</li>
<li><strong>工作原理</strong>：在每个时间步，网络根据当前输入和前一时刻的隐藏状态计算当前隐藏状态，并据此产生输出。</li>
<li><strong>优缺点</strong>：能够处理序列数据，但处理长序列时容易遇到梯度消失或梯度爆炸问题。</li>
</ul>
<h5 id="2-LSTM（长短期记忆网络）"><a href="#2-LSTM（长短期记忆网络）" class="headerlink" title="2. LSTM（长短期记忆网络）"></a>2. LSTM（长短期记忆网络）</h5><ul>
<li><strong>结构特点</strong>：引入特殊的记忆单元（LSTM单元），包括遗忘门、输入门、输出门和细胞状态。</li>
<li><strong>工作原理</strong>：通过门控机制控制信息的流动和更新，解决梯度消失或梯度爆炸问题。</li>
<li><strong>优点</strong>：能够学习到长距离的时序依赖关系。</li>
</ul>
<h5 id="3-GRU（门控循环单元）"><a href="#3-GRU（门控循环单元）" class="headerlink" title="3. GRU（门控循环单元）"></a>3. GRU（门控循环单元）</h5><ul>
<li><strong>结构特点</strong>：LSTM的简化版本，只有两个门：更新门和重置门。</li>
<li><strong>工作原理</strong>：通过更新门和重置门控制信息的流动和更新。</li>
<li><strong>优点</strong>：在保证性能的同时降低了模型的复杂度和计算成本。</li>
</ul>
<h5 id="4-Bi-RNN（双向循环神经网络）"><a href="#4-Bi-RNN（双向循环神经网络）" class="headerlink" title="4. Bi-RNN（双向循环神经网络）"></a>4. Bi-RNN（双向循环神经网络）</h5><ul>
<li><strong>结构特点</strong>：由两个独立的RNN组成，一个正向处理序列，一个反向处理序列。</li>
<li><strong>工作原理</strong>：将两个RNN的输出合并或拼接，以获取序列数据的前后文信息。</li>
<li><strong>优点</strong>：能够捕捉到序列数据中的前后文信息，提高模型的性能。</li>
</ul>
<p>综上所述，RNN可以按照输入输出结构和内部构造进行多种分类。每种类型的RNN都有其独特的特点和工作原理，适用于不同的应用场景和任务需求。在实际应用中，可以根据具体任务的需求选择合适的RNN类型。</p>
<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>CNN是一种专门用于处理网格数据的神经网络结构，尤其擅长处理图像数据。它通过卷积运算来捕捉图像中的局部特征，并通过池化操作来降低数据的维度和复杂度。CNN的提出基于特征提取的思想，期望网络能够自动学习到图像中的层次化特征表示，从而实现对图像的有效分类、识别和其他相关任务。</p>
<p>CNN的核心在于其卷积层和池化层，这些层允许网络在输入数据上滑动窗口，并应用卷积核来提取特征。卷积层通过卷积运算将输入数据映射到特征空间，生成特征图。这些特征图包含了输入数据的局部特征信息，并且具有平移不变性。池化层则对特征图进行下采样，降低数据的维度和计算量，同时保留重要的特征信息。</p>
<p>CNN的基本结构包括输入层、卷积层、池化层、全连接层和输出层。其中，卷积层和池化层交替出现，用于提取和降维特征；全连接层则用于对提取的特征进行分类或回归；输出层则根据任务需求生成最终的输出。</p>
<p>CNN的卷积运算公式为：</p>
<p>$$<br>y_{ij} &#x3D; \sum_{m}\sum_{n} x_{i+m,j+n} \cdot k_{mn} + b<br>$$</p>
<p>其中，$x$是输入数据，$y$是卷积后的特征图，$k$是卷积核，$b$是偏置项，$i$和$j$表示特征图上的位置，$m$和$n$表示卷积核的大小。</p>
<p>CNN的权重参数是局部连接的，即每个卷积核只与输入数据的一部分区域相连。这种局部连接机制使得CNN能够捕捉到图像中的局部特征，并且具有较少的参数数量。同时，卷积核的权重在输入数据的不同位置上是共享的，这种权重共享机制进一步减少了模型的参数数量，并提高了模型的泛化能力。</p>
<p>CNN的前向传播过程是按照层次结构逐步进行的。在每个卷积层，网络通过卷积运算生成特征图；在每个池化层，网络对特征图进行下采样；在全连接层，网络对提取的特征进行分类或回归；最后，在输出层生成最终的输出。</p>
<p>具体过程如下：</p>
<p>输入层接收数据：CNN的输入层接收图像数据，这些数据可以是原始图像或经过预处理的图像。<br>卷积层提取特征：卷积层通过卷积运算和激活函数（如ReLU）提取输入数据的局部特征，并生成特征图。这些特征图包含了输入数据的层次化特征表示。<br>池化层降维：池化层对特征图进行下采样，降低数据的维度和计算量，同时保留重要的特征信息。常见的池化操作包括最大池化和平均池化。<br>全连接层分类或回归：全连接层接收池化层输出的特征向量，并通过权重矩阵和激活函数进行分类或回归任务。<br>输出层生成输出：输出层根据全连接层的输出生成最终的输出，如分类结果、回归值或其他任务相关的输出。</p>
<h4 id="一、按网络结构分类"><a href="#一、按网络结构分类" class="headerlink" title="一、按网络结构分类"></a>一、按网络结构分类</h4><h5 id="1-LeNet"><a href="#1-LeNet" class="headerlink" title="1. LeNet"></a>1. LeNet</h5><ul>
<li><strong>结构特点</strong>：较早的CNN结构，包括卷积层、池化层和全连接层。</li>
<li><strong>工作原理</strong>：通过卷积和池化操作提取图像特征，并通过全连接层进行分类。</li>
<li><strong>应用场景</strong>：手写数字识别等简单图像分类任务。</li>
</ul>
<h5 id="2-AlexNet"><a href="#2-AlexNet" class="headerlink" title="2. AlexNet"></a>2. AlexNet</h5><ul>
<li><strong>结构特点</strong>：包含多个卷积层和池化层，以及较大的全连接层。</li>
<li><strong>工作原理</strong>：通过更深的网络结构提取更复杂的图像特征，提高分类性能。</li>
<li><strong>应用场景</strong>：大规模图像分类任务，如ImageNet竞赛。</li>
</ul>
<h5 id="3-VGG"><a href="#3-VGG" class="headerlink" title="3. VGG"></a>3. VGG</h5><ul>
<li><strong>结构特点</strong>：采用较小的卷积核（如3x3）和较深的网络结构（如VGG-16、VGG-19）。</li>
<li><strong>工作原理</strong>：通过堆叠多个小卷积核来提取图像特征，并增加网络的深度以提高性能。</li>
<li><strong>应用场景</strong>：图像分类、目标检测等任务。</li>
</ul>
<h5 id="4-ResNet（残差网络）"><a href="#4-ResNet（残差网络）" class="headerlink" title="4. ResNet（残差网络）"></a>4. ResNet（残差网络）</h5><ul>
<li><strong>结构特点</strong>：引入残差块，通过跳跃连接解决深层网络的梯度消失问题。</li>
<li><strong>工作原理</strong>：残差块允许网络学习恒等映射，从而更容易地训练深层网络。</li>
<li><strong>应用场景</strong>：图像分类、目标检测、图像分割等任务。</li>
</ul>
<h5 id="5-DenseNet（密集连接网络）"><a href="#5-DenseNet（密集连接网络）" class="headerlink" title="5. DenseNet（密集连接网络）"></a>5. DenseNet（密集连接网络）</h5><ul>
<li><strong>结构特点</strong>：每一层都接收来自前面所有层的特征图作为输入。</li>
<li><strong>工作原理</strong>：通过密集连接实现特征的重用和梯度流动，提高网络的性能。</li>
<li><strong>应用场景</strong>：图像分类、目标检测、图像生成等任务。</li>
</ul>
<h4 id="二、按功能和应用分类"><a href="#二、按功能和应用分类" class="headerlink" title="二、按功能和应用分类"></a>二、按功能和应用分类</h4><h5 id="1-图像分类"><a href="#1-图像分类" class="headerlink" title="1. 图像分类"></a>1. 图像分类</h5><ul>
<li><strong>结构特点</strong>：通常包含多个卷积层、池化层和全连接层。</li>
<li><strong>工作原理</strong>：通过卷积和池化操作提取图像特征，并通过全连接层进行分类。</li>
<li><strong>应用场景</strong>：如ImageNet竞赛、动物识别、植物识别等。</li>
</ul>
<h5 id="2-目标检测"><a href="#2-目标检测" class="headerlink" title="2. 目标检测"></a>2. 目标检测</h5><ul>
<li><strong>结构特点</strong>：在图像分类的基础上增加区域候选网络（RPN）或YOLO等检测头。</li>
<li><strong>工作原理</strong>：通过卷积层提取特征，并通过检测头生成目标的位置和类别信息。</li>
<li><strong>应用场景</strong>：自动驾驶、人脸识别、安防监控等。</li>
</ul>
<h5 id="3-图像分割"><a href="#3-图像分割" class="headerlink" title="3. 图像分割"></a>3. 图像分割</h5><ul>
<li><strong>结构特点</strong>：采用全卷积网络（FCN）或U-Net等结构。</li>
<li><strong>工作原理</strong>：通过卷积层提取特征，并通过上采样操作恢复图像的分辨率，实现像素级别的分类。</li>
<li><strong>应用场景</strong>：医学影像分析、自动驾驶中的道路识别、遥感图像分析等。</li>
</ul>
<h5 id="4-图像生成"><a href="#4-图像生成" class="headerlink" title="4. 图像生成"></a>4. 图像生成</h5><ul>
<li><strong>结构特点</strong>：采用生成对抗网络（GAN）、变分自编码器（VAE）等结构。</li>
<li><strong>工作原理</strong>：通过生成器和判别器的对抗训练，生成逼真的图像或视频。</li>
<li><strong>应用场景</strong>：图像修复、图像超分辨率、视频生成等。</li>
</ul>
<p>综上所述，CNN可以按照网络结构和功能应用进行多种分类。每种类型的CNN都有其独特的特点和工作原理，适用于不同的应用场景和任务需求。在实际应用中，可以根据具体任务的需求选择合适的CNN类型。</p>
<h3 id="递归神经网络（Recursive-Neural-Network-RecNN）"><a href="#递归神经网络（Recursive-Neural-Network-RecNN）" class="headerlink" title="递归神经网络（Recursive Neural Network, RecNN）"></a>递归神经网络（Recursive Neural Network, RecNN）</h3><p>递归神经网络是一种专门用于处理具有层次结构或树形结构数据的神经网络结构。与循环神经网络（RNN）不同，递归神经网络通过递归地应用相同的网络结构来处理不同层次的输入数据，从而能够捕捉数据中的层次信息和结构信息。递归神经网络在自然语言处理、图像解析和其他涉及层次结构数据的任务中表现出色。</p>
<p>递归神经网络的核心在于其递归结构和信息传递机制。这些结构允许网络在输入数据的层次结构上递归地展开，并通过节点间的信息传递来捕捉层次关系。每个节点都代表一个递归神经网络的实例，它接收来自其子节点的信息，并处理这些信息以生成输出，该输出随后被传递给其父节点。这种递归机制使得网络能够处理任意深度的层次结构数据。</p>
<p>递归神经网络的基本结构包括输入层、递归层（或称为隐藏层，但在这里强调其递归性质）和输出层。其中，递归层是核心部分，它负责在层次结构上递归地处理数据。输入层接收层次结构数据的根节点或初始节点作为输入，而输出层则根据任务需求生成最终的输出，如分类结果、解析树或其他相关输出。</p>
<p>递归神经网络的信息传递过程是通过节点间的连接实现的。每个节点都维护一个内部状态，该状态用于存储从子节点接收到的信息以及节点自身的处理结果。当处理一个节点时，网络会将其子节点的输出作为输入，并通过递归层的处理来更新节点的内部状态。然后，该状态被用于生成节点的输出，该输出随后被传递给其父节点。</p>
<p>具体过程如下：</p>
<ol>
<li><p><strong>输入层接收数据</strong>：递归神经网络的输入层接收具有层次结构的数据，如解析树、语法树或其他形式的树形结构数据。</p>
</li>
<li><p><strong>递归层处理数据</strong>：在递归层中，网络对每个节点进行递归处理。对于每个节点，网络会将其子节点的输出作为输入，并通过递归神经网络的运算来更新节点的内部状态。这个状态包含了从子节点接收到的信息以及节点自身的处理结果。然后，网络使用该状态来生成节点的输出。</p>
</li>
<li><p><strong>输出层生成输出</strong>：在输出层中，网络根据递归层生成的节点输出来生成最终的输出。这个输出可以是分类结果、解析树的结构或其他与任务相关的输出。</p>
</li>
</ol>
<p>递归神经网络在处理具有层次结构的数据时具有显著的优势。它们能够捕捉数据中的层次关系和结构信息，从而实现对数据的更深入理解。然而，递归神经网络也面临一些挑战，如梯度消失或爆炸问题，这可能导致网络难以训练。为了解决这个问题，研究人员提出了多种改进方法，如使用门控机制、引入注意力机制或采用更复杂的网络结构。</p>
<p>尽管递归神经网络在自然语言处理和其他涉及层次结构数据的任务中取得了显著成果，但它们的应用仍然受到一些限制。例如，对于非常深的层次结构数据，递归神经网络可能会面临计算复杂性和内存消耗的挑战。此外，由于递归神经网络的结构是树形的，它们可能不适用于处理具有循环或网状结构的数据。</p>
<p>综上所述，递归神经网络是一种专门用于处理具有层次结构数据的神经网络结构。它们通过递归地应用相同的网络结构来处理不同层次的输入数据，从而能够捕捉数据中的层次信息和结构信息。尽管面临一些挑战和限制，递归神经网络在自然语言处理和其他相关任务中仍然具有广泛的应用前景。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://Lavoisier7.github.io">Lavoisier</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://lavoisier7.github.io/2024/11/22/Neural_Network/">http://lavoisier7.github.io/2024/11/22/Neural_Network/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://Lavoisier7.github.io" target="_blank">D</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/NN/">NN</a></div><div class="post-share"><div class="social-share" data-image="/img/Yoimiya.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/11/21/Book_Review2/" title="书评特辑·第二期"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">书评特辑·第二期</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2024/11/24/Marcel_Proust/" title="Marcel Proust"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Marcel Proust</div></div><div class="info-2"><div class="info-item-1">前半生出入社交名利场  后半生缠绵病榻； 长期遭受慢性失眠症的折磨 ...</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/Yoimiya.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Lavoisier</div><div class="author-info-description">世故消耗激情，岁月打磨棱角，现实浪掷理想。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Lavoisier7"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Lavoisier7" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wzd6@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">世故消耗激情，岁月打磨棱角，现实浪掷理想。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">神经网络简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.</span> <span class="toc-text">什么是神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2"><span class="toc-number">1.2.</span> <span class="toc-text">神经网络的发展历史</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86"><span class="toc-number">1.3.</span> <span class="toc-text">神经网络的组成部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8ELLM%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">1.4.</span> <span class="toc-text">神经网络与LLM的关系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NN%E5%88%86%E7%B1%BB%E5%8F%8A%E5%BA%94%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">NN分类及应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.1.</span> <span class="toc-text">前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN"><span class="toc-number">2.2.</span> <span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%8C%89%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%84%E5%88%86%E7%B1%BB"><span class="toc-number">2.2.1.</span> <span class="toc-text">一、按输入输出结构分类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-N-vs-N-RNN"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">1. N vs N - RNN</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-N-vs-1-RNN"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">2. N vs 1 - RNN</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-vs-N-RNN"><span class="toc-number">2.2.1.3.</span> <span class="toc-text">3. 1 vs N - RNN</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-N-vs-M-RNN%EF%BC%88seq2seq%EF%BC%89"><span class="toc-number">2.2.1.4.</span> <span class="toc-text">4. N vs M - RNN（seq2seq）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%8C%89%E5%86%85%E9%83%A8%E6%9E%84%E9%80%A0%E5%88%86%E7%B1%BB"><span class="toc-number">2.2.2.</span> <span class="toc-text">二、按内部构造分类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E4%BC%A0%E7%BB%9FRNN"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">1. 传统RNN</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-LSTM%EF%BC%88%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="toc-number">2.2.2.2.</span> <span class="toc-text">2. LSTM（长短期记忆网络）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-GRU%EF%BC%88%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83%EF%BC%89"><span class="toc-number">2.2.2.3.</span> <span class="toc-text">3. GRU（门控循环单元）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-Bi-RNN%EF%BC%88%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="toc-number">2.2.2.4.</span> <span class="toc-text">4. Bi-RNN（双向循环神经网络）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN"><span class="toc-number">2.3.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%8C%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%88%86%E7%B1%BB"><span class="toc-number">2.3.1.</span> <span class="toc-text">一、按网络结构分类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-LeNet"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">1. LeNet</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-AlexNet"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">2. AlexNet</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-VGG"><span class="toc-number">2.3.1.3.</span> <span class="toc-text">3. VGG</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-ResNet%EF%BC%88%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="toc-number">2.3.1.4.</span> <span class="toc-text">4. ResNet（残差网络）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-DenseNet%EF%BC%88%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="toc-number">2.3.1.5.</span> <span class="toc-text">5. DenseNet（密集连接网络）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%8C%89%E5%8A%9F%E8%83%BD%E5%92%8C%E5%BA%94%E7%94%A8%E5%88%86%E7%B1%BB"><span class="toc-number">2.3.2.</span> <span class="toc-text">二、按功能和应用分类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">1. 图像分类</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">2. 目标检测</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">3. 图像分割</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="toc-number">2.3.2.4.</span> <span class="toc-text">4. 图像生成</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Recursive-Neural-Network-RecNN%EF%BC%89"><span class="toc-number">2.4.</span> <span class="toc-text">递归神经网络（Recursive Neural Network, RecNN）</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/04/Report_System/" title="本站评论使用手册">本站评论使用手册</a><time datetime="2025-08-04T07:51:47.000Z" title="发表于 2025-08-04 15:51:47">2025-08-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/03/Procedural_Justice/" title="当程序正义无法抵达事实的彼岸">当程序正义无法抵达事实的彼岸</a><time datetime="2025-08-03T12:34:01.000Z" title="发表于 2025-08-03 20:34:01">2025-08-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/06/Debate_Competition/" title="Awsome Debate Competition">Awsome Debate Competition</a><time datetime="2025-05-06T11:06:31.000Z" title="发表于 2025-05-06 19:06:31">2025-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/06/Compiler_Principle/" title="Compiler Principle">Compiler Principle</a><time datetime="2025-05-05T17:02:13.000Z" title="发表于 2025-05-06 01:02:13">2025-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/06/Principle_of_Computer_Organization/" title="Computer Organization">Computer Organization</a><time datetime="2025-05-05T16:40:57.000Z" title="发表于 2025-05-06 00:40:57">2025-05-06</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/bow.jpg);"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By Lavoisier</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text">直觉让我回了头，却没能让我转过身。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  let initFn = window.walineFn || null
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = {"requiredMeta":["nick","mail"],"locale":null,"placeholder":"昵称和邮箱为必填项，为了您能及时收到相关回复的邮件通知，请确保邮箱的正确性！"}

  const destroyWaline = ele => ele.destroy()

  const initWaline = (Fn, el = document, path = window.location.pathname) => {
    const waline = Fn({
      el: el.querySelector('#waline-wrap'),
      serverURL: 'https://report.lavoisier7.xyz/',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      comment: true,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    if (isShuoshuo) {
      window.shuoshuoComment.destroyWaline = () => {
        destroyWaline(waline)
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const loadWaline = (el, path) => {
    if (initFn) initWaline(initFn, el, path)
    else {
      btf.getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css')
        .then(() => import('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js'))
        .then(({ init }) => {
          initFn = init || Waline.init
          initWaline(initFn, el, path)
          window.walineFn = initFn
        })
    }
  }

  if (isShuoshuo) {
    'Waline' === 'Waline'
      ? window.shuoshuoComment = { loadComment: loadWaline } 
      : window.loadOtherComment = loadWaline
    return
  }

  if ('Waline' === 'Waline' || !false) {
    if (false) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
    else setTimeout(loadWaline, 0)
  } else {
    window.loadOtherComment = loadWaline
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>